{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82605564-a4df-4a02-b3b8-662d2ce85a8a",
   "metadata": {},
   "source": [
    "### Analytics Cup 2024 - Max Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf9a05f-3f50-49d7-a8be-b49e9bc9e807",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Set up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5311b3-441f-471e-bbad-6546655a583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the Packages\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572554a8-08e7-4393-be17-5ce8305c7f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the Seed (pandas, statsmodels, matplotlib and y_data_profiling rely on numpy's random generator, and thus, we need to set the seed in numpy\n",
    "seed = 2024\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabda2b-e497-4f4c-8a0e-e7bbd8dc8256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data \n",
    "\n",
    "# Importing the data \n",
    "diet_df = pd.read_csv(\"diet.csv\")\n",
    "recipes_df = pd.read_csv(\"recipes.csv\")\n",
    "requests_df = pd.read_csv(\"requests.csv\")\n",
    "reviews_df = pd.read_csv(\"reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e72a46-434b-44b6-8596-cbf73919c5a5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### **Data Understanding and Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5cbc38-dfd4-466f-8ac2-f54c665089fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Notes: \n",
    "# One diet per AuthorId\n",
    "# One recipe per RecipeId \n",
    "# Several requests per author (But only one request per author per recipe) \n",
    "# Several reviews per author (But only one review per author per recipe) \n",
    "\n",
    "# 1) Merged diets and reviews -> Dataset with reviews and diet information \n",
    "# 2) Merged (diets & reviews) with requests on AuthorId and RecipeId\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd54a5c-d954-4081-8925-833092bbcb90",
   "metadata": {},
   "source": [
    "#### Diet Understanding âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203eb16d-4fc3-4b4d-bf27-71aec88abfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get overview of the diet dataset.\n",
    "print(diet_df.head())\n",
    "print()\n",
    "print(diet_df.info())\n",
    "print()\n",
    "print(diet_df.isnull().sum()) # --> 1 missing value in the \"Diet\" column\n",
    "print()\n",
    "\n",
    "# Row with the missing value: \n",
    "print(diet_df[diet_df.isnull().any(axis=1)])\n",
    "# --> AuthorId: 5, Diet: NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65c8a0c-068c-43c0-b51b-10717d0817da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the diet dataset\n",
    "print(diet_df.describe())\n",
    "\n",
    "\n",
    "\n",
    "# Create the boxplot for the age column\n",
    "sns.boxplot(y=diet_df[\"Age\"], color=\"skyblue\")  # Set the color\n",
    "plt.title(\"Age Boxplot\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Create a barplot for the diet column\n",
    "ax = sns.countplot(x='Diet', data=diet_df)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc2d1d-dce7-418d-9ef5-ff138118f7c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show how many unique Authors there are in the diet dataset\n",
    "print(diet_df['AuthorId'].nunique()) # --> 271907 unique authors --> Every row is a unique author\n",
    "print(diet_df['AuthorId'].duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b61844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Insights: \n",
    "# - 3 Attirubtes\n",
    "# - In the diet column we have 1 row with a null value \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ff674",
   "metadata": {},
   "source": [
    "#### Diet Cleaning âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31bd8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the row in the diet dataset with the missing value. \n",
    "diet_df = diet_df.dropna() # Potential implications: When merging we loose a row of data. \n",
    "\n",
    "# Make the \"Diet\" column of type category & dummy variable encode it.\n",
    "diet_cleaned = pd.get_dummies(diet_df, columns=['Diet'], drop_first=True)\n",
    "\n",
    "diet_cleaned\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b4b269-35d7-4b4d-85f1-f66280860caa",
   "metadata": {},
   "source": [
    "#### Recipes Understanding ðŸš§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac762b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Recipe Overview\n",
    "display(recipes_df.head())\n",
    "print()\n",
    "# print(recipes_df.info())\n",
    "#print()\n",
    "print(recipes_df.isnull().sum()) # --> No missing values in the ProteinContent column, 26713 missing values in the RecipeServings column, 50295 missing values in the RecipeYield column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab8246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the Cook Time\n",
    "\n",
    "# Missing values in the cooktime column\n",
    "print(recipes_df[recipes_df['CookTime'].isnull()]) # --> 0 missing values in the CookTime column\n",
    "\n",
    "# Describe the cooktime column\n",
    "print(recipes_df['CookTime'].describe())\n",
    "\n",
    "# Boxplot of Cooktime \n",
    "sns.boxplot(y=recipes_df[\"CookTime\"], color=\"skyblue\") # The values are very far apart\n",
    "\n",
    "\n",
    "# Problems: \n",
    "# - Values incredibly far apart and don't make sense.\n",
    "# - Probably in seconds? \n",
    "\n",
    "# What to do? \n",
    "# - Delete abnormalities \n",
    "# - Normalize it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d6fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the Prep Time\n",
    "\n",
    "# Missing values in the PrepTime column\n",
    "print(recipes_df[recipes_df['PrepTime'].isnull()]) # --> 0 missing values in the PrepTime column\n",
    "\n",
    "# Describe the PrepTime column\n",
    "print(recipes_df['PrepTime'].describe())\n",
    "\n",
    "# Boxplot of PrepTime\n",
    "sns.boxplot(y=recipes_df[\"PrepTime\"], color=\"skyblue\") # Their are outliers. But most values around the bottom. \n",
    "\n",
    "# Problems: \n",
    "# - Values incredibly far apart and don't make sense.\n",
    "# - Probably in seconds? \n",
    "\n",
    "# What to do? \n",
    "# - Delete outliers \n",
    "# - Normalize it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91fc4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the Recipe Category\n",
    "\n",
    "# Missing values in the RecipeCategory column\n",
    "print(recipes_df[recipes_df['RecipeCategory'].isnull()]) # --> 0 missing values in the RecipeCategory column\n",
    "\n",
    "# Number of values in the RecipeCategory column\n",
    "print(recipes_df['RecipeCategory'].value_counts()) \n",
    "print(sns.countplot(x='RecipeCategory', data=recipes_df)); # 7 --> unique Recipe Categoreis (most of them in other)\n",
    "\n",
    "# Problems: \n",
    "# - Recipes in the Category \"Other\" don't tell us much\n",
    "# - We also don't have \"Dinner\". \n",
    "\n",
    "# What to do?\n",
    "# - Somehow split up the other column \n",
    "# - Delete the other column\n",
    "# - This column with the requests and see what categories could make sense. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c408b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the \"RecipeIngredientQuantities\" column\n",
    "\n",
    "# Missing values in the RecipeIngredientQuantities column\n",
    "print(recipes_df[recipes_df['RecipeIngredientQuantities'].isnull()]) # --> 0 missing values in the RecipeIngredientQuantities column\n",
    "\n",
    "# Number of values in the RecipeIngredientQuantities column\n",
    "print(recipes_df['RecipeIngredientQuantities'].value_counts()) # --> 1 --> unique RecipeIngredientQuantities\n",
    "\n",
    "# Insights: \n",
    "# - Not all values are unique (e.g. \"\\\"1\\\"\", \"\\\"1\\\"\", \"\\\"1\\\"\" exists 211 times)\"\n",
    "\n",
    "\n",
    "# Problems:\n",
    "\n",
    "# What to do?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce005a5",
   "metadata": {},
   "source": [
    "#### Recipe Cleaning ðŸš§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e556045",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_cleaned = recipes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da833b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling (Dropping) Name Column\n",
    "\n",
    "recipes_cleaned = recipes_df.drop(columns=['Name'])\n",
    "recipes_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02a9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling CookTime and PrepTime Column\n",
    "\n",
    "# TODO: Handle outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b327bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hanndle RecipeCategory Column\n",
    "recipes_cleaned = pd.get_dummies(recipes_cleaned, columns=['RecipeCategory'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dd8b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling RecipeIngredientQuantities Column (TEMPORARY -> TODO: FIX)\n",
    "\n",
    "recipes_cleaned = recipes_cleaned.drop(columns=['RecipeIngredientQuantities'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling RecipeIngredientParts Column (TEMPORARY -> TODO: FIX)\n",
    "\n",
    "recipes_cleaned = recipes_cleaned.drop(columns=['RecipeIngredientParts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a534c7e-5f99-456b-a94e-d9dcefee4c57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Handle Nutritional Facts Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298647b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle RecipeServings Column (Temporary: TODO: FIX)\n",
    "\n",
    "recipes_cleaned = recipes_cleaned.drop(columns=['RecipeServings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db653c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle RecipeYield Column (Temporary: TODO: FIX)\n",
    "\n",
    "recipes_cleaned = recipes_cleaned.drop(columns=['RecipeYield'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1366311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca843dd-ca91-4684-b36d-b9c11beec324",
   "metadata": {},
   "source": [
    "#### Requests Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1543b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Request Insights: \n",
    "# - No missing values \n",
    "\n",
    "# - 90847 duplicate authors --> More than one request per author\n",
    "# - 0 duplicate AuthorID + RecipeID combinations --> Every author has only one request per recipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e71a5c-9d9b-4bbc-86bd-6ee2f761e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Request Overview\n",
    "display(requests_df.head())\n",
    "print()\n",
    "# print(requests_df.isnull().sum()) # --> No missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188a3ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the numerical columns\n",
    "requests_df.describe()\n",
    "requests_cleaned = requests_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7102b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicate Authors\n",
    "print(requests_df['AuthorId'].duplicated().sum()) # --> 90847 duplicate authors --> More than one request per author\n",
    "\n",
    "print(requests_df.duplicated(subset=['AuthorId', 'RecipeId']).sum()) # 0 duplicate AuthorID + RecipeID combinations --> Every author has only one request per recipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff6f2e",
   "metadata": {},
   "source": [
    "#### Requests Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8d17d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the \"Time\" Column\n",
    "requests_cleaned[\"Time\"] = requests_df[\"Time\"].round().astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25e811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the HighCalories Column \n",
    "requests_cleaned['HighCalories'] = requests_df['HighCalories'].astype('int')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56fc2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the HighProtein Column (1: Yes High Protein, 0: I don't care)\n",
    "requests_cleaned['HighProtein'] = requests_df['HighProtein'].map({'Yes': 1, 'Indifferent': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a421cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the LowFat Column (Nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f79d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the LowSugar Column (1: Yes low sugar, 0: I don't give a shit)\n",
    "requests_cleaned['LowSugar'] = requests_df['LowSugar'].map({'0': 1, 'Indifferent': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d961a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the HighFiber Column (Nothing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfd1cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c24623-88a0-438a-a211-b84cf4aeca12",
   "metadata": {},
   "source": [
    "#### Reviews Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d709377-1cd3-4b71-ac83-fcfe07d95ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Reviews Overview: \n",
    "display(reviews_df.head())\n",
    "print()\n",
    "print(reviews_df.isnull().sum()) # --> Missing values in columns, \"Rating\", \"Like\" and \"TestSetId\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f5f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicate Reviews\n",
    "print(reviews_df['AuthorId'].duplicated().sum()) # --> 90847 duplicate authors --> More than one review per author\n",
    "print(requests_df.duplicated(subset=['AuthorId', 'RecipeId']).sum()) # 0 duplicate AuthorID + ReviewId combinations --> Every author has only one review per recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3462daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique values in the \"Rating\" column\n",
    "print(reviews_df[\"Rating\"].value_counts()) # We either haven NaNs or Rating: 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c2e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique values in the \"Like\" column: \n",
    "print(reviews_df['Like'].value_counts()); # --> make true = 1 and false = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a18776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Dataset into Modeling and Submission\n",
    "\n",
    "reviews_df_cleaned = reviews_df\n",
    "\n",
    "# reviews_df\n",
    "\n",
    "# reviews_df_submission = reviews_df[reviews_df[\"Like\"].isnull()]\n",
    "\n",
    "# reviews_df_modelling = reviews_df[reviews_df[\"Like\"].notnull()]\n",
    "\n",
    "# What's the deal here? \n",
    "# - reviews_df -> 140195 rows \n",
    "# - reviews_df_submission -> 42814 rows \n",
    "# - reviews_df_modelling -> 97381 rows97381 rows \n",
    "\n",
    "# Was hat Marcel mit der TestSetId gemacht?\n",
    "\n",
    "# print(reviews_df.shape[0])\n",
    "# print(reviews_df_submission.shape[0])\n",
    "# print(reviews_df_modelling.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43675da-32b6-4468-9f03-dfb63723507d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Reviews Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fe2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling \"Rating\" Column (dropping it)\n",
    "\n",
    "reviews_df_cleaned = reviews_df_cleaned.drop(columns=['Rating'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a1b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling \"Like\" Column (1: True, 0: False)\n",
    "reviews_df_cleaned['Like'] = reviews_df_cleaned['Like'].map({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea33fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling \"TestSetId\" Column (Dropping it)\n",
    "\n",
    "#reviews_df_cleaned = reviews_df_cleaned.drop(columns=['TestSetId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb065593",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11ff58f-cf11-40d2-abb3-d47ab203da70",
   "metadata": {},
   "source": [
    "#### **Data Merging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd88f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.merge(reviews_df_cleaned, diet_cleaned, on=\"AuthorId\")\n",
    "full_df = pd.merge(full_df, requests_cleaned, on=[\"AuthorId\", \"RecipeId\"])\n",
    "full_df = pd.merge(full_df, recipes_cleaned, on=\"RecipeId\")\n",
    "\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d3f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Dataset into Modeling and Submission\n",
    "\n",
    "full_df_submission = full_df[full_df[\"Like\"].isnull()]\n",
    "full_df_modelling = full_df[full_df[\"Like\"].notnull()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171a168d",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d1b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression: \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a6ec3d-8d73-4fe7-ab20-3eaf317bb5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splliting the data into training and testing data \n",
    "\n",
    "X = full_df_modelling.drop([\"AuthorId\", \"RecipeId\", \"Like\", \"TestSetId\"], axis=1)\n",
    "y = full_df_modelling[\"Like\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "  train_test_split(X, y,\n",
    "                   test_size=0.3, \n",
    "                   shuffle=True,\n",
    "                   random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modeling \n",
    "\n",
    "logreg_model = LogisticRegression(max_iter=30)\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "955ca8b4b2c479ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predicting\n",
    "\n",
    "predictions = logreg_model.predict(X_test)\n",
    "\n",
    "count_1s = np.count_nonzero(predictions == 1)\n",
    "count_0s = np.count_nonzero(predictions == 0)\n",
    "\n",
    "print(f\"Number of 1s: {count_1s}\")\n",
    "print(f\"Number of 0s: {count_0s}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef914a45198162e4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluating the model \n",
    "\n",
    "print(balanced_accuracy_score(y_test, predictions))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3fd5ffc2bea7703"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest: \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Fit Random Forest Classifier model\n",
    "rf_model = RandomForestClassifier(n_estimators=5, random_state=seed)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(\"classification report\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# Calculate balanced accuracy\n",
    "print(\"balanced accuracy score\")\n",
    "print(balanced_accuracy_score(y_test, predictions))\n",
    "\n",
    "count_1s = np.count_nonzero(predictions == 1)\n",
    "count_0s = np.count_nonzero(predictions == 0)\n",
    "\n",
    "print()\n",
    "print(f\"Number of 1s: {count_1s}\")\n",
    "print(f\"Number of 0s: {count_0s}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a6e628aeef74bfd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Finding best Classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc699d1be835ec65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#   1. LogisticRegression --> scale or increase number of iterations!!\n",
    "#   2. RandomForestClassifier\n",
    "#   3. GradientBoostingClassifier\n",
    "    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model_logistic_regression = LogisticRegression(max_iter=10)\n",
    "model_random_forest = RandomForestClassifier()\n",
    "model_gradient_boosting = GradientBoostingClassifier()\n",
    "\n",
    "# train the models\n",
    "pipeline = Pipeline(steps=[(\"model\", None)])\n",
    "\n",
    "#parameter_grid_preprocessing = {\n",
    "#  \"pca__n_components\" : [1, 2, 3, 4],\n",
    "#}\n",
    "\n",
    "parameter_grid_logistic_regression = {\n",
    "  \"model\" : [model_logistic_regression],\n",
    "  \"model__C\" : [0.1, 1, 10],  # inverse regularization strength\n",
    "}\n",
    "\n",
    "parameter_grid_gradient_boosting = {\n",
    "  \"model\" : [model_gradient_boosting],\n",
    "  \"model__n_estimators\" : [10, 20, 30, 50]\n",
    "}\n",
    "\n",
    "parameter_grid_random_forest = {\n",
    "  \"model\" : [model_random_forest],\n",
    "  \"model__n_estimators\" : [1, 2, 3, 4, 5, 20, 50, 80],  # number of max trees in the forest\n",
    "  \"model__max_depth\" : [None, 50, 100],\n",
    "}\n",
    "\n",
    "meta_parameter_grid = [#parameter_grid_logistic_regression,\n",
    "                       parameter_grid_random_forest\n",
    "                       ,parameter_grid_gradient_boosting\n",
    "]\n",
    "\n",
    "#meta_parameter_grid = [{**parameter_grid_preprocessing, **model_grid}\n",
    "#                       for model_grid in meta_parameter_grid]\n",
    "\n",
    "search = GridSearchCV(pipeline,\n",
    "                      meta_parameter_grid, \n",
    "                      scoring=\"balanced_accuracy\",\n",
    "                      n_jobs=2, \n",
    "                      cv=5,  # number of folds for cross-validation \n",
    "                      error_score=\"raise\"\n",
    ")\n",
    "# here, the actual training and grid search happens\n",
    "search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print(\"best parameter:\", search.best_params_ ,\"(CV score=%0.3f)\" % search.best_score_)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c92b75ffaa35c130"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for a detailed look on the performance of the different models\n",
    "def get_search_score_overview():\n",
    "  for c,s in zip(search.cv_results_[\"params\"],search.cv_results_[\"mean_test_score\"]):\n",
    "      print(c, s)\n",
    "\n",
    "print(get_search_score_overview())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c01eba2cb0b16ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Interpretability"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T20:28:04.325388Z",
     "start_time": "2024-01-15T20:28:04.287915Z"
    }
   },
   "id": "44afdb025b1e0be8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import shap\n",
    "# assume random forest model\n",
    "model = RandomForestClassifier(n_estimators=5, random_state=seed)\n",
    "model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# compute shapley values\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "shap_interaction_values = explainer.shap_interaction_values(X_train)\n",
    "\n",
    "expected_value = explainer.expected_value\n",
    "print(expected_value)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "285d2c308d2bc2e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# class dependent plots of shapley values for each feature\n",
    "for i,c in enumerate(full_df_modelling.variety.unique()):\n",
    "    shap.summary_plot(shap_values[i], X_train, show=False)\n",
    "    plt.title(\"Shapley values for \"+str(c))\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55bd7760cbf1cc0f"
  },
  {
   "cell_type": "markdown",
   "id": "161cb5d6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T20:23:26.293124Z"
    }
   },
   "source": [
    "## Generating the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325075cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "submission_predictions = rf_model.predict(full_df_submission.drop([\"AuthorId\", \"RecipeId\", \"Like\", \"TestSetId\"], axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e30e6ec-2390-4086-b828-54f41d654be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'id': full_df_submission.TestSetId.astype(int), 'prediction': submission_predictions.astype(int)})\n",
    "\n",
    "output = output.sort_values('id')\n",
    "#output = output.reindex(columns=[\"id\", \"prediction\"])\n",
    "\n",
    "output.to_csv('../predictions_analytics_acrobots_1.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
