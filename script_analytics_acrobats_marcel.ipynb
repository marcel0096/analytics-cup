{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82605564-a4df-4a02-b3b8-662d2ce85a8a",
   "metadata": {},
   "source": [
    "### Analytics Cup 2024 - Marcel Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf9a05f-3f50-49d7-a8be-b49e9bc9e807",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Set up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5311b3-441f-471e-bbad-6546655a583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the Packages\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572554a8-08e7-4393-be17-5ce8305c7f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the Seed (pandas, statsmodels, matplotlib and y_data_profiling rely on numpy's random generator, and thus, we need to set the seed in numpy\n",
    "seed = 2024\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabda2b-e497-4f4c-8a0e-e7bbd8dc8256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data \n",
    "\n",
    "# Importing the data \n",
    "diet_df = pd.read_csv(\"diet.csv\")\n",
    "recipes_df = pd.read_csv(\"recipes.csv\")\n",
    "requests_df = pd.read_csv(\"requests.csv\")\n",
    "reviews_df = pd.read_csv(\"reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e72a46-434b-44b6-8596-cbf73919c5a5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### **Data Understanding and Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5cbc38-dfd4-466f-8ac2-f54c665089fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Notes: \n",
    "# One diet per AuthorId\n",
    "# One recipe per RecipeId \n",
    "# Several requests per author (But only one request per author per recipe) \n",
    "# Several reviews per author (But only one review per author per recipe) \n",
    "\n",
    "# 1) Merged diets and reviews -> Dataset with reviews and diet information \n",
    "# 2) Merged (diets & reviews) with requests on AuthorId and RecipeId\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd54a5c-d954-4081-8925-833092bbcb90",
   "metadata": {},
   "source": [
    "#### Diet Understanding âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203eb16d-4fc3-4b4d-bf27-71aec88abfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get overview of the diet dataset.\n",
    "print(diet_df.head())\n",
    "print()\n",
    "print(diet_df.info())\n",
    "print()\n",
    "print(diet_df.isnull().sum()) # --> 1 missing value in the \"Diet\" column\n",
    "print()\n",
    "\n",
    "# Row with the missing value: \n",
    "print(diet_df[diet_df.isnull().any(axis=1)])\n",
    "# --> AuthorId: 5, Diet: NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65c8a0c-068c-43c0-b51b-10717d0817da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the diet dataset\n",
    "print(diet_df.describe())\n",
    "\n",
    "\n",
    "\n",
    "# Create the boxplot for the age column\n",
    "sns.boxplot(y=diet_df[\"Age\"], color=\"skyblue\")  # Set the color\n",
    "plt.title(\"Age Boxplot\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Create a barplot for the diet column\n",
    "ax = sns.countplot(x='Diet', data=diet_df)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc2d1d-dce7-418d-9ef5-ff138118f7c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show how many unique Authors there are in the diet dataset\n",
    "print(diet_df['AuthorId'].nunique()) # --> 271907 unique authors --> Every row is a unique author\n",
    "print(diet_df['AuthorId'].duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b61844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Insights: \n",
    "# - 3 Attirubtes\n",
    "# - In the diet column we have 1 row with a null value \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ff674",
   "metadata": {},
   "source": [
    "#### Diet Cleaning âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31bd8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the row in the diet dataset with the missing value. \n",
    "diet_df = diet_df.dropna() # Potential implications: When merging we loose a row of data. \n",
    "\n",
    "# Make the \"Diet\" column of type category & dummy variable encode it.\n",
    "diet_cleaned = pd.get_dummies(diet_df, columns=['Diet'], drop_first=True)\n",
    "\n",
    "diet_cleaned\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b4b269-35d7-4b4d-85f1-f66280860caa",
   "metadata": {},
   "source": [
    "#### Recipes Understanding ðŸš§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac762b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Recipe Overview\n",
    "display(recipes_df.head())\n",
    "print()\n",
    "# print(recipes_df.info())\n",
    "#print()\n",
    "print(recipes_df.isnull().sum()) # --> No missing values in the ProteinContent column, 26713 missing values in the RecipeServings column, 50295 missing values in the RecipeYield column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab8246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the Cook Time\n",
    "\n",
    "# Missing values in the cooktime column\n",
    "print(recipes_df[recipes_df['CookTime'].isnull()]) # --> 0 missing values in the CookTime column\n",
    "\n",
    "# Describe the cooktime column\n",
    "print(recipes_df['CookTime'].describe())\n",
    "\n",
    "# Boxplot of Cooktime \n",
    "sns.boxplot(y=recipes_df[\"CookTime\"], color=\"skyblue\") # The values are very far apart\n",
    "\n",
    "\n",
    "# Problems: \n",
    "# - Values incredibly far apart and don't make sense.\n",
    "# - Probably in seconds? \n",
    "\n",
    "# What to do? \n",
    "# - Delete abnormalities \n",
    "# - Normalize it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d6fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the Prep Time\n",
    "\n",
    "# Missing values in the PrepTime column\n",
    "print(recipes_df[recipes_df['PrepTime'].isnull()]) # --> 0 missing values in the PrepTime column\n",
    "\n",
    "# Describe the PrepTime column\n",
    "print(recipes_df['PrepTime'].describe())\n",
    "\n",
    "# Boxplot of PrepTime\n",
    "sns.boxplot(y=recipes_df[\"PrepTime\"], color=\"skyblue\") # Their are outliers. But most values around the bottom. \n",
    "\n",
    "# Problems: \n",
    "# - Values incredibly far apart and don't make sense.\n",
    "# - Probably in seconds? \n",
    "\n",
    "# What to do? \n",
    "# - Delete outliers \n",
    "# - Normalize it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91fc4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the Recipe Category\n",
    "\n",
    "# Missing values in the RecipeCategory column\n",
    "print(recipes_df[recipes_df['RecipeCategory'].isnull()]) # --> 0 missing values in the RecipeCategory column\n",
    "\n",
    "# Number of values in the RecipeCategory column\n",
    "print(recipes_df['RecipeCategory'].value_counts()) \n",
    "print(sns.countplot(x='RecipeCategory', data=recipes_df)); # 7 --> unique Recipe Categoreis (most of them in other)\n",
    "\n",
    "# Problems: \n",
    "# - Recipes in the Category \"Other\" don't tell us much\n",
    "# - We also don't have \"Dinner\". \n",
    "\n",
    "# What to do?\n",
    "# - Somehow split up the other column \n",
    "# - Delete the other column\n",
    "# - This column with the requests and see what categories could make sense. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c408b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the \"RecipeIngredientQuantities\" column\n",
    "\n",
    "# Missing values in the RecipeIngredientQuantities column\n",
    "print(recipes_df[recipes_df['RecipeIngredientQuantities'].isnull()]) # --> 0 missing values in the RecipeIngredientQuantities column\n",
    "\n",
    "# Number of values in the RecipeIngredientQuantities column\n",
    "print(recipes_df['RecipeIngredientQuantities'].value_counts()) # --> 1 --> unique RecipeIngredientQuantities\n",
    "\n",
    "# Insights: \n",
    "# - Not all values are unique (e.g. \"\\\"1\\\"\", \"\\\"1\\\"\", \"\\\"1\\\"\" exists 211 times)\"\n",
    "\n",
    "\n",
    "# Problems:\n",
    "\n",
    "# What to do?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce005a5",
   "metadata": {},
   "source": [
    "#### Recipe Cleaning ðŸš§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e556045",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_cleaned = recipes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da833b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling (Dropping) Name Column\n",
    "\n",
    "recipes_cleaned = recipes_df.drop(columns=['Name'])\n",
    "recipes_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02a9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling CookTime and PrepTime Column\n",
    "\n",
    "# TODO: Handle outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b327bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hanndle RecipeCategory Column\n",
    "recipes_cleaned = pd.get_dummies(recipes_cleaned, columns=['RecipeCategory'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dd8b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling RecipeIngredientQuantities Column (TEMPORARY -> TODO: FIX)\n",
    "\n",
    "#recipes_cleaned = recipes_cleaned.drop(columns=['RecipeIngredientQuantities'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling RecipeIngredientParts Column (TEMPORARY -> TODO: FIX)\n",
    "\n",
    "#recipes_cleaned = recipes_cleaned.drop(columns=['RecipeIngredientParts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a534c7e-5f99-456b-a94e-d9dcefee4c57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Handle Nutritional Facts Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298647b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle RecipeServings Column (Temporary: TODO: FIX)\n",
    "\n",
    "recipes_cleaned = recipes_cleaned.drop(columns=['RecipeServings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db653c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle RecipeYield Column (Temporary: TODO: FIX)\n",
    "\n",
    "recipes_cleaned = recipes_cleaned.drop(columns=['RecipeYield'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1366311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5f1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column that counts the number of ingredients per row\n",
    "\n",
    "recipes_cleaned['Number of Ingredients'] = recipes_cleaned['RecipeIngredientParts'].str.count(',') + 1\n",
    "\n",
    "# also create column that counts the number of quantities per row\n",
    "\n",
    "recipes_cleaned['Number of Quantities'] = recipes_cleaned['RecipeIngredientQuantities'].str.count(',') + 1\n",
    "\n",
    "recipes_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35a8318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the RecipeIngredientParts column, look for parts that contain meat, so 'chicken', 'beef', etc.\n",
    "\n",
    "recipes_cleaned['Meat'] = recipes_cleaned['RecipeIngredientParts'].str.contains('chicken|beef|pork|lamb|turkey|duck|goose|fish|seafood|salmon|shrimp|crab|lobster|ham|bacon|sausage|meat|steak|veal|venison|bison|liver|lamb|poultry|meat')\n",
    "recipes_cleaned['Meat'] = recipes_cleaned['Meat'].astype(int)\n",
    "\n",
    "#recipes_test['Meat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fabdcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the RecipeIngredientParts column, look for parts that contain vegetables, so 'carrot', 'potato', etc.\n",
    "\n",
    "recipes_cleaned['Vegetables'] = recipes_cleaned['RecipeIngredientParts'].str.contains('carrot|potato|tomato|onion|garlic|broccoli|spinach|cucumber|lettuce|celery|cabbage|cauliflower|corn|pepper|peas|beans|asparagus|eggplant|zucchini|squash|pumpkin|radish|beet|turnip|sprouts|vegetable|vegetables|kale|bell pepper|sweet potato|cherry tomato|artichoke|bok choy|brussels sprouts|butternut squash|leek|okra|snow peas|snap peas|green beans|fennel|watercress|arugula|chard|collard greens|endive|escarole|kohlrabi|mustard greens|pattypan squash|romanesco broccoli|rutabaga|sugar snap peas|swiss chard|tatsoi|water chestnut|yam|daikon|lotus root|bamboo shoots|jicama|plantain|radicchio|turnip greens|water spinach|bitter melon|chayote|chicory|dandelion greens|edamame|lotus root|nopales|portobello mushroom|shiitake mushroom|maitake mushroom|enoki mushroom|oyster mushroom|mushroom|zucchini blossom|artichoke|sunchoke|beets|cardoon|salsify|kale|endive')\n",
    "recipes_cleaned['Vegetables'] = recipes_cleaned['Vegetables'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b6c0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the RecipeIngredientParts column, look for parts that contain dairy products, so 'milk', 'cheese', etc.\n",
    "\n",
    "recipes_cleaned['Dairy'] = recipes_cleaned['RecipeIngredientParts'].str.contains('milk|cheese|yogurt|cream|butter|buttermilk|sour cream|cottage cheese|mascarpone|ricotta|cream cheese|mozzarella|cheddar|brie|feta|parmesan|gouda|havarti|blue cheese|goat cheese|swiss cheese|provolone|muenster cheese|asiago|colby jack|mozzarella sticks|queso fresco|queso blanco|halloumi|neufchÃ¢tel|ghee|clotted cream')\n",
    "recipes_cleaned['Dairy'] = recipes_cleaned['Dairy'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ddf4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the RecipeIngredientParts column, look for parts that contain fruits, so 'apple', 'banana', etc.\n",
    "\n",
    "recipes_cleaned['Fruits'] = recipes_cleaned['RecipeIngredientParts'].str.contains('apple|banana|orange|grape|strawberry|blueberry|raspberry|blackberry|cherry|peach|pear|plum|pineapple|kiwi|lemon|lime|grapefruit|cantaloupe|watermelon|pomegranate|nectarine|apricot|fig|guava|lychee|mango|papaya|persimmon|star fruit|tangerine|dragon fruit|passion fruit|jackfruit|durian|breadfruit|quince|cranberry|currant|elderberry|gooseberry|boysenberry|lingonberry|mulberry|olive|date|prune|raisin|clementine|kumquat|tangelo|blood orange|carambola|loquat|ugli fruit|breadfruit|cherimoya|custard apple|guanabana|guava|horned melon|jujube|kiwano|kumquat|longan|lychee|mangosteen|marionberry|miracle fruit|papaya|persimmon|pomelo|quince|rambutan|sapodilla|tamarillo|tamarind|ugli fruit|yuzu|acai berry|ackee|araza|barbadine|bilberry|black sapote|blackcurrant|blueberry|boysenberry|breadfruit|canistel|cempedak|cherry|cloudberry|coconut|cranberry|cupuacu|currant|damson plum|durian|elderberry|feijoa|fig|goji berry|gooseberry|grape|grapefruit|guava|honeyberry|huckleberry|jabuticaba|jackfruit|jambul|jujube|juniper berry|kiwano|kiwi|kumquat|lemon|lime|lingonberry|loganberry|longan|loquat|lychee|mandarin orange|mango|marionberry|melon|mulberry|nectarine')\n",
    "recipes_cleaned['Fruits'] = recipes_cleaned['Fruits'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01eb0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the RecipeIngredientParts column, look for parts that contain alcohol \n",
    "\n",
    "recipes_cleaned['Alcohol'] = recipes_cleaned['RecipeIngredientParts'].str.contains('beer|wine|vodka|gin|rum|tequila|whiskey|brandy|bourbon|scotch|cognac|liqueur|vermouth|sherry|port|sake|champagne|prosecco|cider|mead|absinthe|amaretto|aperol|aquavit|armagnac|bitters|campari|chartreuse|cointreau|creme de cacao|creme de menthe|curacao|drambuie|frangelico|grand marnier|kahlua|lillet|limoncello|maraschino|midori|pastis|pisco|sambuca|schnapps|sloe gin|st. germain|triple sec|vermouth|absinthe|amaretto|aperol|aquavit|armagnac|bitters|campari|chartreuse|cointreau|creme de cacao|creme de menthe|curacao|drambuie|frangelico|grand marnier|kahlua|lillet|limoncello|maraschino|midori|pastis|pisco|sambuca|schnapps|sloe gin|st. germain|triple sec|vermouth')\n",
    "recipes_cleaned['Alcohol'] = recipes_cleaned['Alcohol'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7a2bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f590cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of rows where the number of ingredients and quantities do not match\n",
    "\n",
    "print(recipes_cleaned['Number of Ingredients'].ne(recipes_cleaned['Number of Quantities']).sum())\n",
    "\n",
    "# count the number of rows where the number of ingredients and quantities are more than 2 apart\n",
    "\n",
    "print(recipes_cleaned['Number of Ingredients'].sub(recipes_cleaned['Number of Quantities']).abs().gt(3).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8332ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to create a complexity value for each recipe\n",
    "\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "recipes_cleaned['Complexity'] = (0.2 * recipes_test['Number of Ingredients']) + (0.4 * recipes_test['PrepTime']) + (0.4 * recipes_test['CookTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a8918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224ec0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop weird columns RecipeIngredientQuantities and RecipeIngredientParts\n",
    "\n",
    "recipes_cleaned = recipes_cleaned.drop(columns=['RecipeIngredientQuantities', 'RecipeIngredientParts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge recipes_test with recipes_df on RecipeId\n",
    "\n",
    "#recipes_cleaned = pd.merge(recipes_test, recipes_cleaned, on=['RecipeId'])\n",
    "\n",
    "recipes_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccba0aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further ideas\n",
    "# - classify meals into very easy, easy, medium, hard, very hard based on ingredient amounts and cooking/prep times -> normalize first\n",
    "# - sum up quantities and multiply with parts to get another feature \n",
    "# - classify ingredients: meat, vegetables, nuts, gluten, dairy, fruit, spices, herbs, alcohol, \n",
    "\n",
    "# Is the transformation of Time correct? Both in recipes and requests? Is it comparable with the new complexity column?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca843dd-ca91-4684-b36d-b9c11beec324",
   "metadata": {},
   "source": [
    "#### Requests Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1543b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Request Insights: \n",
    "# - No missing values \n",
    "\n",
    "# - 90847 duplicate authors --> More than one request per author\n",
    "# - 0 duplicate AuthorID + RecipeID combinations --> Every author has only one request per recipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e71a5c-9d9b-4bbc-86bd-6ee2f761e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Request Overview\n",
    "display(requests_df.head())\n",
    "print()\n",
    "# print(requests_df.isnull().sum()) # --> No missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188a3ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the numerical columns\n",
    "requests_df.describe()\n",
    "requests_cleaned = requests_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7102b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicate Authors\n",
    "print(requests_df['AuthorId'].duplicated().sum()) # --> 90847 duplicate authors --> More than one request per author\n",
    "\n",
    "print(requests_df.duplicated(subset=['AuthorId', 'RecipeId']).sum()) # 0 duplicate AuthorID + RecipeID combinations --> Every author has only one request per recipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff6f2e",
   "metadata": {},
   "source": [
    "#### Requests Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8d17d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the \"Time\" Column\n",
    "requests_cleaned[\"Time\"] = requests_df[\"Time\"].round().astype(int)\n",
    "\n",
    "# also transform it with Scaler as in recipes\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#requests_cleaned[['Time']] = scaler.fit_transform(requests_cleaned[['Time']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25e811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the HighCalories Column \n",
    "requests_cleaned['HighCalories'] = requests_df['HighCalories'].astype('int')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56fc2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the HighProtein Column (1: Yes High Protein, 0: I don't care)\n",
    "requests_cleaned['HighProtein'] = requests_df['HighProtein'].map({'Yes': 1, 'Indifferent': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a421cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the LowFat Column (Nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f79d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the LowSugar Column (1: Yes low sugar, 0: I don't give a shit)\n",
    "requests_cleaned['LowSugar'] = requests_df['LowSugar'].map({'0': 1, 'Indifferent': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d961a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the HighFiber Column (Nothing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfd1cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c24623-88a0-438a-a211-b84cf4aeca12",
   "metadata": {},
   "source": [
    "#### Reviews Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d709377-1cd3-4b71-ac83-fcfe07d95ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Reviews Overview: \n",
    "display(reviews_df.head())\n",
    "print()\n",
    "print(reviews_df.isnull().sum()) # --> Missing values in columns, \"Rating\", \"Like\" and \"TestSetId\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f5f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicate Reviews\n",
    "print(reviews_df['AuthorId'].duplicated().sum()) # --> 90847 duplicate authors --> More than one review per author\n",
    "print(requests_df.duplicated(subset=['AuthorId', 'RecipeId']).sum()) # 0 duplicate AuthorID + ReviewId combinations --> Every author has only one review per recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3462daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique values in the \"Rating\" column\n",
    "print(reviews_df[\"Rating\"].value_counts()) # We either haven NaNs or Rating: 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c2e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique values in the \"Like\" column: \n",
    "print(reviews_df['Like'].value_counts()); # --> make true = 1 and false = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a18776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Dataset into Modeling and Submission -> not yet since we need the full one fpr merging\n",
    "\n",
    "reviews_df_cleaned = reviews_df\n",
    "\n",
    "# reviews_df\n",
    "\n",
    "# reviews_df_submission = reviews_df[reviews_df[\"Like\"].isnull()]\n",
    "\n",
    "# reviews_df_modelling = reviews_df[reviews_df[\"Like\"].notnull()]\n",
    "\n",
    "# What's the deal here? \n",
    "# - reviews_df -> 140195 rows \n",
    "# - reviews_df_submission -> 42814 rows \n",
    "# - reviews_df_modelling -> 97381 rows97381 rows \n",
    "\n",
    "# Was hat Marcel mit der TestSetId gemacht?\n",
    "\n",
    "# print(reviews_df.shape[0])\n",
    "# print(reviews_df_submission.shape[0])\n",
    "# print(reviews_df_modelling.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43675da-32b6-4468-9f03-dfb63723507d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Reviews Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fe2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling \"Rating\" Column (dropping it)\n",
    "\n",
    "reviews_df_cleaned = reviews_df_cleaned.drop(columns=['Rating'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a1b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling \"Like\" Column (1: True, 0: False)\n",
    "reviews_df_cleaned['Like'] = reviews_df_cleaned['Like'].map({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea33fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling \"TestSetId\" Column (Dropping it)\n",
    "\n",
    "#reviews_df_cleaned = reviews_df_cleaned.drop(columns=['TestSetId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb065593",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11ff58f-cf11-40d2-abb3-d47ab203da70",
   "metadata": {},
   "source": [
    "#### **Data Merging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd88f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.merge(reviews_df_cleaned, diet_cleaned, on=\"AuthorId\")\n",
    "full_df = pd.merge(full_df, requests_cleaned, on=[\"AuthorId\", \"RecipeId\"])\n",
    "full_df = pd.merge(full_df, recipes_cleaned, on=\"RecipeId\")\n",
    "\n",
    "# drop non normalized CookTime and PrepTime columns\n",
    "#full_df = full_df.drop(columns=['CookTime', 'PrepTime'])\n",
    "\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d3f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Dataset into Modeling and Submission\n",
    "\n",
    "full_df_submission = full_df[full_df[\"Like\"].isnull()]\n",
    "full_df_modelling = full_df[full_df[\"Like\"].notnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b8d5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_modelling.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d304d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select column Time, CookTime and PrepTime, Complexity, Number of Ingredients by their name\n",
    "\n",
    "#full_df_modelling = full_df_modelling[['Time', 'CookTime', 'PrepTime', 'Complexity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de9e1b3",
   "metadata": {},
   "source": [
    "#### Scaling non-binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d8cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# Scale all columns where it makes sense and select them\n",
    "\n",
    "columns_to_scale = ['Age', 'Time', 'CookTime', 'PrepTime', 'Calories', 'FatContent', 'SaturatedFatContent', 'CholesterolContent', 'SodiumContent', 'CarbohydrateContent', \n",
    "                    'FiberContent', 'SugarContent', 'ProteinContent', 'Complexity', 'Number of Ingredients', 'Number of Quantities']\n",
    "\n",
    "full_df_modelling[columns_to_scale] = robust_scaler.fit_transform(full_df_modelling[columns_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a49d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171a168d",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d1b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression: \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a6ec3d-8d73-4fe7-ab20-3eaf317bb5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splliting the data into training and testing data \n",
    "\n",
    "X = full_df_modelling.drop([\"AuthorId\", \"RecipeId\", \"Like\", \"TestSetId\"], axis=1)\n",
    "y = full_df_modelling[\"Like\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "  train_test_split(X, y,\n",
    "                   test_size=0.3, \n",
    "                   shuffle=True,\n",
    "                   random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e6b69a-967d-492d-a337-6bcf5d9ee40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling \n",
    "\n",
    "logreg_model = LogisticRegression(max_iter=30)\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0caf0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting\n",
    "\n",
    "predictions = logreg_model.predict(X_test)\n",
    "\n",
    "count_1s = np.count_nonzero(predictions == 1)\n",
    "count_0s = np.count_nonzero(predictions == 0)\n",
    "\n",
    "print(f\"Number of 1s: {count_1s}\")\n",
    "print(f\"Number of 0s: {count_0s}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eca0433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model \n",
    "\n",
    "print(balanced_accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b4f982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Fit Random Forest Classifier model\n",
    "rf_model = RandomForestClassifier(n_estimators=5, random_state=seed)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(\"classification report\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# Calculate balanced accuracy\n",
    "print(\"balanced accuracy score\")\n",
    "print(balanced_accuracy_score(y_test, predictions))\n",
    "\n",
    "count_1s = np.count_nonzero(predictions == 1)\n",
    "count_0s = np.count_nonzero(predictions == 0)\n",
    "\n",
    "print()\n",
    "print(f\"Number of 1s: {count_1s}\")\n",
    "print(f\"Number of 0s: {count_0s}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4fca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "\n",
    "#   1. LogisticRegression --> scale or increase number of iterations!!\n",
    "#   2. RandomForestClassifier\n",
    "#   3. GradientBoostingClassifier\n",
    "    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model_logistic_regression = LogisticRegression(max_iter=10)\n",
    "model_random_forest = RandomForestClassifier()\n",
    "model_gradient_boosting = GradientBoostingClassifier()\n",
    "\n",
    "# train the models\n",
    "pipeline = Pipeline(steps=[#(\"scaler\", standard_scaler),\n",
    "                           \n",
    "                           (\"model\", None)])\n",
    "\n",
    "#parameter_grid_preprocessing = {\n",
    "#  \"pca__n_components\" : [1, 2, 3, 4],\n",
    "#}\n",
    "\n",
    "parameter_grid_logistic_regression = {\n",
    "  \"model\" : [model_logistic_regression],\n",
    "  \"model__C\" : [0.1, 1, 10],  # inverse regularization strength\n",
    "}\n",
    "\n",
    "parameter_grid_gradient_boosting = {\n",
    "  \"model\" : [model_gradient_boosting],\n",
    "  \"model__n_estimators\" : [10, 20, 30, 50]\n",
    "}\n",
    "\n",
    "parameter_grid_random_forest = {\n",
    "  \"model\" : [model_random_forest],\n",
    "  \"model__n_estimators\" : [1, 2, 3, 4, 5, 20, 50, 80],  # number of max trees in the forest\n",
    "  \"model__max_depth\" : [None, 50, 100],\n",
    "}\n",
    "\n",
    "meta_parameter_grid = [#parameter_grid_logistic_regression,\n",
    "                       parameter_grid_random_forest\n",
    "                       ,parameter_grid_gradient_boosting\n",
    "]\n",
    "\n",
    "#meta_parameter_grid = [{**parameter_grid_preprocessing, **model_grid}\n",
    "#                       for model_grid in meta_parameter_grid]\n",
    "\n",
    "search = GridSearchCV(pipeline,\n",
    "                      meta_parameter_grid, \n",
    "                      scoring=\"balanced_accuracy\",\n",
    "                      n_jobs=4, \n",
    "                      cv=5,  # number of folds for cross-validation \n",
    "                      error_score=\"raise\"\n",
    ")\n",
    "# here, the actual training and grid search happens\n",
    "search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print(\"best parameter:\", search.best_params_ ,\"(CV score=%0.3f)\" % search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161cb5d6",
   "metadata": {},
   "source": [
    "## Generating the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325075cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "\n",
    "#submission_predictions = rf_model.predict(full_df_submission.drop([\"AuthorId\", \"RecipeId\", \"Like\", \"TestSetId\"], axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e30e6ec-2390-4086-b828-54f41d654be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = pd.DataFrame({'id': full_df_submission.TestSetId.astype(int), 'prediction': submission_predictions.astype(int)})\n",
    "\n",
    "#output = output.sort_values('id')\n",
    "#output = output.reindex(columns=[\"id\", \"prediction\"])\n",
    "\n",
    "#output.to_csv('../predictions_analytics_acrobots_1.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
